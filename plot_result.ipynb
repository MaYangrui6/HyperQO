{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T08:29:49.013856Z",
     "start_time": "2024-02-29T08:28:38.824384Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27503/1544516627.py:40: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  init.uniform(param)\n",
      "/tmp/ipykernel_27503/1544516627.py:38: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(param)\n",
      "2024-03-01 12:44:33,948 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-03-01 12:44:34,198 : INFO : adding document #10000 to Dictionary<134 unique tokens: ['Index_Scan_customer_demographics_Filter_cd_gender=ANDcd_marital_status=ANDcd_education_status=Index_Cond_cd_demo_sk=store_sales.ss_cdemo_sk', 'Seq_Scan_date_dim_Filter_d_year=', 'Seq_Scan_promotion_Filter_p_channel_email=ORp_channel_event=', 'Index_Scan_customer_address_Filter_ca_country=ANDca_state=ANYORca_state=ANYORca_state=ANYANDca_state=ANYANDstore_sales.ss_net_profit>=ANDstore_sales.ss_net_profit<=ORca_state=ANYANDstore_sales.ss_net_profit>=ANDstore_sales.ss_net_profit<=ORca_state=ANYANDstore_sales.ss_net_profit>=ANDstore_sales.ss_net_profit<=Index_Cond_ca_address_sk=store_sales.ss_addr_sk', 'Index_Scan_customer_demographics_Filter_cd_marital_status=ANDcd_education_status=ORcd_marital_status=ANDcd_education_status=ORcd_marital_status=ANDcd_education_status=ANDcd_marital_status=ANDcd_education_status=ANDstore_sales.ss_sales_price>=.ANDstore_sales.ss_sales_price<=.ORcd_marital_status=ANDcd_education_status=ANDstore_sales.ss_sales_price>=.ANDstore_sales.ss_sales_price<=.ORcd_marital_status=ANDcd_education_status=ANDstore_sales.ss_sales_price>=.ANDstore_sales.ss_sales_price<=.Index_Cond_cd_demo_sk=store_sales.ss_cdemo_sk']...>\n",
      "2024-03-01 12:44:34,327 : INFO : built Dictionary<134 unique tokens: ['Index_Scan_customer_demographics_Filter_cd_gender=ANDcd_marital_status=ANDcd_education_status=Index_Cond_cd_demo_sk=store_sales.ss_cdemo_sk', 'Seq_Scan_date_dim_Filter_d_year=', 'Seq_Scan_promotion_Filter_p_channel_email=ORp_channel_event=', 'Index_Scan_customer_address_Filter_ca_country=ANDca_state=ANYORca_state=ANYORca_state=ANYANDca_state=ANYANDstore_sales.ss_net_profit>=ANDstore_sales.ss_net_profit<=ORca_state=ANYANDstore_sales.ss_net_profit>=ANDstore_sales.ss_net_profit<=ORca_state=ANYANDstore_sales.ss_net_profit>=ANDstore_sales.ss_net_profit<=Index_Cond_ca_address_sk=store_sales.ss_addr_sk', 'Index_Scan_customer_demographics_Filter_cd_marital_status=ANDcd_education_status=ORcd_marital_status=ANDcd_education_status=ORcd_marital_status=ANDcd_education_status=ANDcd_marital_status=ANDcd_education_status=ANDstore_sales.ss_sales_price>=.ANDstore_sales.ss_sales_price<=.ORcd_marital_status=ANDcd_education_status=ANDstore_sales.ss_sales_price>=.ANDstore_sales.ss_sales_price<=.ORcd_marital_status=ANDcd_education_status=ANDstore_sales.ss_sales_price>=.ANDstore_sales.ss_sales_price<=.Index_Cond_cd_demo_sk=store_sales.ss_cdemo_sk']...> from 14400 documents (total 51200 corpus positions)\n",
      "2024-03-01 12:44:34,351 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<134 unique tokens: ['Index_Scan_customer_demographics_Filter_cd_gender=ANDcd_marital_status=ANDcd_education_status=Index_Cond_cd_demo_sk=store_sales.ss_cdemo_sk', 'Seq_Scan_date_dim_Filter_d_year=', 'Seq_Scan_promotion_Filter_p_channel_email=ORp_channel_event=', 'Index_Scan_customer_address_Filter_ca_country=ANDca_state=ANYORca_state=ANYORca_state=ANYANDca_state=ANYANDstore_sales.ss_net_profit>=ANDstore_sales.ss_net_profit<=ORca_state=ANYANDstore_sales.ss_net_profit>=ANDstore_sales.ss_net_profit<=ORca_state=ANYANDstore_sales.ss_net_profit>=ANDstore_sales.ss_net_profit<=Index_Cond_ca_address_sk=store_sales.ss_addr_sk', 'Index_Scan_customer_demographics_Filter_cd_marital_status=ANDcd_education_status=ORcd_marital_status=ANDcd_education_status=ORcd_marital_status=ANDcd_education_status=ANDcd_marital_status=ANDcd_education_status=ANDstore_sales.ss_sales_price>=.ANDstore_sales.ss_sales_price<=.ORcd_marital_status=ANDcd_education_status=ANDstore_sales.ss_sales_price>=.ANDstore_sales.ss_sales_price<=.ORcd_marital_status=ANDcd_education_status=ANDstore_sales.ss_sales_price>=.ANDstore_sales.ss_sales_price<=.Index_Cond_cd_demo_sk=store_sales.ss_cdemo_sk']...> from 14400 documents (total 51200 corpus positions)\", 'datetime': '2024-03-01T12:44:34.351574', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]', 'platform': 'Linux-4.19.0-9.ucloud-x86_64-with-glibc2.27', 'event': 'created'}\n",
      "2024-03-01 12:44:34,659 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d20,n5,w5,mc3,s0.001,t3>', 'datetime': '2024-03-01T12:44:34.659219', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]', 'platform': 'Linux-4.19.0-9.ucloud-x86_64-with-glibc2.27', 'event': 'created'}\n",
      "2024-03-01 12:44:34,667 : INFO : collecting all words and their counts\n",
      "2024-03-01 12:44:34,676 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2024-03-01 12:44:34,720 : INFO : PROGRESS: at example #10000, processed 35466 words (865622 words/s), 134 word types, 0 tags\n",
      "2024-03-01 12:44:34,764 : INFO : collected 134 word types and 14400 unique tags from a corpus of 14400 examples and 51200 words\n",
      "2024-03-01 12:44:34,777 : INFO : Creating a fresh vocabulary\n",
      "2024-03-01 12:44:34,781 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 134 unique words (100.00% of original 134, drops 0)', 'datetime': '2024-03-01T12:44:34.780970', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]', 'platform': 'Linux-4.19.0-9.ucloud-x86_64-with-glibc2.27', 'event': 'prepare_vocab'}\n",
      "2024-03-01 12:44:34,786 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 51200 word corpus (100.00% of original 51200, drops 0)', 'datetime': '2024-03-01T12:44:34.786203', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]', 'platform': 'Linux-4.19.0-9.ucloud-x86_64-with-glibc2.27', 'event': 'prepare_vocab'}\n",
      "2024-03-01 12:44:34,789 : INFO : deleting the raw counts dictionary of 134 items\n",
      "2024-03-01 12:44:34,799 : INFO : sample=0.001 downsamples 127 most-common words\n",
      "2024-03-01 12:44:34,801 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 23695.320384657447 word corpus (46.3%% of prior 51200)', 'datetime': '2024-03-01T12:44:34.801876', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]', 'platform': 'Linux-4.19.0-9.ucloud-x86_64-with-glibc2.27', 'event': 'prepare_vocab'}\n",
      "2024-03-01 12:44:34,808 : INFO : estimated required memory for 134 words and 20 dimensions: 4120440 bytes\n",
      "2024-03-01 12:44:34,817 : INFO : resetting layer weights\n",
      "2024-03-01 12:44:34,836 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 134 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-03-01T12:44:34.836535', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]', 'platform': 'Linux-4.19.0-9.ucloud-x86_64-with-glibc2.27', 'event': 'train'}\n",
      "2024-03-01 12:44:36,117 : INFO : EPOCH 0 - PROGRESS: at 80.89% examples, 25177 words/s, in_qsize 1, out_qsize 1\n",
      "2024-03-01 12:44:36,129 : INFO : EPOCH 0: training on 51200 raw words (38092 effective words) took 1.2s, 30867 effective words/s\n",
      "2024-03-01 12:44:37,082 : INFO : EPOCH 1: training on 51200 raw words (38076 effective words) took 0.9s, 41817 effective words/s\n",
      "2024-03-01 12:44:38,004 : INFO : EPOCH 2: training on 51200 raw words (38161 effective words) took 0.9s, 41824 effective words/s\n",
      "2024-03-01 12:44:38,892 : INFO : EPOCH 3: training on 51200 raw words (38158 effective words) took 0.8s, 44975 effective words/s\n",
      "2024-03-01 12:44:39,853 : INFO : EPOCH 4: training on 51200 raw words (38031 effective words) took 0.9s, 40340 effective words/s\n",
      "2024-03-01 12:44:40,777 : INFO : EPOCH 5: training on 51200 raw words (38028 effective words) took 0.9s, 42125 effective words/s\n",
      "2024-03-01 12:44:41,679 : INFO : EPOCH 6: training on 51200 raw words (38134 effective words) took 0.9s, 43642 effective words/s\n",
      "2024-03-01 12:44:42,621 : INFO : EPOCH 7: training on 51200 raw words (38149 effective words) took 0.9s, 41469 effective words/s\n",
      "2024-03-01 12:44:43,486 : INFO : EPOCH 8: training on 51200 raw words (38133 effective words) took 0.8s, 45775 effective words/s\n",
      "2024-03-01 12:44:44,383 : INFO : EPOCH 9: training on 51200 raw words (38055 effective words) took 0.9s, 43631 effective words/s\n",
      "2024-03-01 12:44:45,348 : INFO : EPOCH 10: training on 51200 raw words (37933 effective words) took 0.9s, 40785 effective words/s\n",
      "2024-03-01 12:44:46,256 : INFO : EPOCH 11: training on 51200 raw words (38020 effective words) took 0.9s, 43217 effective words/s\n",
      "2024-03-01 12:44:47,143 : INFO : EPOCH 12: training on 51200 raw words (38260 effective words) took 0.9s, 44593 effective words/s\n",
      "2024-03-01 12:44:48,047 : INFO : EPOCH 13: training on 51200 raw words (38088 effective words) took 0.9s, 43767 effective words/s\n",
      "2024-03-01 12:44:49,031 : INFO : EPOCH 14: training on 51200 raw words (38013 effective words) took 1.0s, 39716 effective words/s\n",
      "2024-03-01 12:44:49,930 : INFO : EPOCH 15: training on 51200 raw words (38147 effective words) took 0.9s, 44386 effective words/s\n",
      "2024-03-01 12:44:50,803 : INFO : EPOCH 16: training on 51200 raw words (38090 effective words) took 0.8s, 45173 effective words/s\n",
      "2024-03-01 12:44:51,747 : INFO : EPOCH 17: training on 51200 raw words (38357 effective words) took 0.9s, 41683 effective words/s\n",
      "2024-03-01 12:44:52,679 : INFO : EPOCH 18: training on 51200 raw words (38127 effective words) took 0.9s, 42689 effective words/s\n",
      "2024-03-01 12:44:53,565 : INFO : EPOCH 19: training on 51200 raw words (38238 effective words) took 0.9s, 44327 effective words/s\n",
      "2024-03-01 12:44:54,469 : INFO : EPOCH 20: training on 51200 raw words (38057 effective words) took 0.9s, 43127 effective words/s\n",
      "2024-03-01 12:44:55,343 : INFO : EPOCH 21: training on 51200 raw words (38314 effective words) took 0.8s, 45499 effective words/s\n",
      "2024-03-01 12:44:56,312 : INFO : EPOCH 22: training on 51200 raw words (38152 effective words) took 0.9s, 40768 effective words/s\n",
      "2024-03-01 12:44:57,288 : INFO : EPOCH 23: training on 51200 raw words (38064 effective words) took 0.9s, 40551 effective words/s\n",
      "2024-03-01 12:44:58,242 : INFO : EPOCH 24: training on 51200 raw words (38034 effective words) took 0.9s, 42313 effective words/s\n",
      "2024-03-01 12:44:59,184 : INFO : EPOCH 25: training on 51200 raw words (38131 effective words) took 0.9s, 41645 effective words/s\n",
      "2024-03-01 12:45:00,170 : INFO : EPOCH 26: training on 51200 raw words (38026 effective words) took 0.9s, 40089 effective words/s\n",
      "2024-03-01 12:45:01,106 : INFO : EPOCH 27: training on 51200 raw words (38011 effective words) took 0.9s, 42237 effective words/s\n",
      "2024-03-01 12:45:02,022 : INFO : EPOCH 28: training on 51200 raw words (37992 effective words) took 0.9s, 43797 effective words/s\n",
      "2024-03-01 12:45:03,075 : INFO : EPOCH 29 - PROGRESS: at 79.95% examples, 29807 words/s, in_qsize 1, out_qsize 1\n",
      "2024-03-01 12:45:03,084 : INFO : EPOCH 29: training on 51200 raw words (38304 effective words) took 1.0s, 36850 effective words/s\n",
      "2024-03-01 12:45:04,096 : INFO : EPOCH 30: training on 51200 raw words (37881 effective words) took 1.0s, 38846 effective words/s\n",
      "2024-03-01 12:45:05,053 : INFO : EPOCH 31: training on 51200 raw words (38165 effective words) took 0.9s, 42008 effective words/s\n",
      "2024-03-01 12:45:06,142 : INFO : EPOCH 32 - PROGRESS: at 80.89% examples, 29646 words/s, in_qsize 1, out_qsize 1\n",
      "2024-03-01 12:45:06,170 : INFO : EPOCH 32: training on 51200 raw words (38050 effective words) took 1.1s, 35753 effective words/s\n",
      "2024-03-01 12:45:07,239 : INFO : EPOCH 33 - PROGRESS: at 79.95% examples, 30064 words/s, in_qsize 1, out_qsize 1\n",
      "2024-03-01 12:45:07,242 : INFO : EPOCH 33: training on 51200 raw words (38102 effective words) took 1.0s, 37477 effective words/s\n",
      "2024-03-01 12:45:08,279 : INFO : EPOCH 34 - PROGRESS: at 79.95% examples, 30020 words/s, in_qsize 1, out_qsize 1\n",
      "2024-03-01 12:45:08,286 : INFO : EPOCH 34: training on 51200 raw words (38023 effective words) took 1.0s, 37196 effective words/s\n",
      "2024-03-01 12:45:09,273 : INFO : EPOCH 35: training on 51200 raw words (38225 effective words) took 0.9s, 40568 effective words/s\n",
      "2024-03-01 12:45:10,240 : INFO : EPOCH 36: training on 51200 raw words (38040 effective words) took 0.9s, 40493 effective words/s\n",
      "2024-03-01 12:45:11,189 : INFO : EPOCH 37: training on 51200 raw words (38135 effective words) took 0.9s, 41785 effective words/s\n",
      "2024-03-01 12:45:12,082 : INFO : EPOCH 38: training on 51200 raw words (38098 effective words) took 0.9s, 43906 effective words/s\n",
      "2024-03-01 12:45:13,001 : INFO : EPOCH 39: training on 51200 raw words (38111 effective words) took 0.9s, 42387 effective words/s\n",
      "2024-03-01 12:45:13,003 : INFO : Doc2Vec lifecycle event {'msg': 'training on 2048000 raw words (1524205 effective words) took 38.2s, 39940 effective words/s', 'datetime': '2024-03-01T12:45:13.003707', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]', 'platform': 'Linux-4.19.0-9.ucloud-x86_64-with-glibc2.27', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>cost_no_index</th>\n",
       "      <th>cost_dta</th>\n",
       "      <th>cost_reduction</th>\n",
       "      <th>cost_reduction_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>select  i_item_id, \\n        avg(ss_quantity) ...</td>\n",
       "      <td>108534.81</td>\n",
       "      <td>69925.31</td>\n",
       "      <td>38609.50</td>\n",
       "      <td>0.355734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>select sum (ss_quantity)\\n from store_sales, s...</td>\n",
       "      <td>147137.68</td>\n",
       "      <td>86652.87</td>\n",
       "      <td>60484.81</td>\n",
       "      <td>0.411076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>select  i_item_id\\n       ,i_item_desc \\n     ...</td>\n",
       "      <td>50155.33</td>\n",
       "      <td>3944.83</td>\n",
       "      <td>46210.50</td>\n",
       "      <td>0.921348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>with ss_items as\\n (select i_item_id item_id\\n...</td>\n",
       "      <td>162566.89</td>\n",
       "      <td>839.12</td>\n",
       "      <td>161727.77</td>\n",
       "      <td>0.994838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>select  asceding.rnk, i1.i_product_name best_p...</td>\n",
       "      <td>274060.77</td>\n",
       "      <td>48.66</td>\n",
       "      <td>274012.11</td>\n",
       "      <td>0.999822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  cost_no_index  cost_dta  \\\n",
       "0  select  i_item_id, \\n        avg(ss_quantity) ...      108534.81  69925.31   \n",
       "1  select sum (ss_quantity)\\n from store_sales, s...      147137.68  86652.87   \n",
       "2  select  i_item_id\\n       ,i_item_desc \\n     ...       50155.33   3944.83   \n",
       "3  with ss_items as\\n (select i_item_id item_id\\n...      162566.89    839.12   \n",
       "4  select  asceding.rnk, i1.i_product_name best_p...      274060.77     48.66   \n",
       "\n",
       "   cost_reduction  cost_reduction_ratio  \n",
       "0        38609.50              0.355734  \n",
       "1        60484.81              0.411076  \n",
       "2        46210.50              0.921348  \n",
       "3       161727.77              0.994838  \n",
       "4       274012.11              0.999822  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import sys\n",
    "from ImportantConfig import Config\n",
    "from sql2fea import TreeBuilder, value_extractor\n",
    "from NET import TreeNet\n",
    "from sql2fea import Sql2Vec\n",
    "from TreeLSTM import SPINN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from PGUtils import pgrunner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sql_feature.workload_embedder import PredicateEmbedderDoc2Vec\n",
    "\n",
    "sys.path.append('/home/ubuntu/project/HyperQO')\n",
    "\n",
    "config = Config()\n",
    "# sys.stdout = open(config.log_file, \"w\")\n",
    "random.seed(0)\n",
    "\n",
    "with open(config.queries_file) as f:\n",
    "    import json\n",
    "\n",
    "    queries = json.load(f)\n",
    "\n",
    "tree_builder = TreeBuilder()\n",
    "sql2vec = Sql2Vec()\n",
    "# 这里的 input_size 必须为偶数！\n",
    "value_network = SPINN(head_num=config.head_num, input_size=36, hidden_size=config.hidden_size, table_num=50,\n",
    "                          sql_size=config.sql_size, attention_dim=30).to(config.device)\n",
    "for name, param in value_network.named_parameters():\n",
    "    from torch.nn import init\n",
    "\n",
    "    if len(param.shape) == 2:\n",
    "        init.xavier_normal(param)\n",
    "    else:\n",
    "        init.uniform(param)\n",
    "\n",
    "treenet_model = TreeNet(tree_builder, value_network)\n",
    "\n",
    "mask = (torch.rand(1, config.head_num, device=config.device) < 0.9).long()\n",
    "\n",
    "train = pd.read_csv('./information/train.csv', index_col=0)\n",
    "queries = train['query'].values\n",
    "\n",
    "workload_embedder = PredicateEmbedderDoc2Vec(queries, 20, pgrunner)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7a61ba-7b15-4b1e-b9cc-4830b715b837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(train.index)\n",
    "y = torch.tensor(train['cost_reduction_ratio'].values)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 例如，均方误差损失\n",
    "optimizer = treenet_model.optimizer  # 例如，Adam 优化器\n",
    "\n",
    "Batch_Size = 32\n",
    "torch_dataset = Data.TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4868a75d-e0bb-48ce-afae-609f9018f3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11520, 2880)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, val_set = train_test_split(torch_dataset, test_size=0.2, shuffle = True)\n",
    "len(train_set),len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bda3b50b-7215-42d4-a61d-fdf1ba8f87e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader_val = Data.DataLoader(dataset=val_set,\n",
    "                                 batch_size=Batch_Size//4,\n",
    "                                 shuffle=True)\n",
    "loader_val=[x for x in loader_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "433be535-f96f-4316-a034-b1aebc9e0f62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 7876,  9119,  7251,  2479,   935,  1411, 10724, 12925]),\n",
       " tensor([0.0769, 0.6167, 0.1156, 0.0727, 0.9403, 0.3241, 0.0397, 0.2933],\n",
       "        dtype=torch.float64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x_val,batch_y_val=loader_val[batch_num]\n",
    "batch_x_val,batch_y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "170bf9f551df759c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T08:29:49.019328Z",
     "start_time": "2024-02-29T08:29:49.019015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m sql_vec \u001b[38;5;241m=\u001b[39m workload_embedder\u001b[38;5;241m.\u001b[39mget_embedding([sql])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m loss, pred_val \u001b[38;5;241m=\u001b[39m treenet_model\u001b[38;5;241m.\u001b[39mtrain(plan_json, sql_vec, target_value, mask,is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m list_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     32\u001b[0m list_pred\u001b[38;5;241m.\u001b[39mappend(pred_val)\n",
      "File \u001b[0;32m~/project/mayang/HyperQO/NET.py:164\u001b[0m, in \u001b[0;36mTreeNet.train\u001b[0;34m(self, plan_json, sql_vec, target_value, mask, is_train)\u001b[0m\n\u001b[1;32m    162\u001b[0m sql_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_network\u001b[38;5;241m.\u001b[39msql_feature(sql_vec)\n\u001b[1;32m    163\u001b[0m pred_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplan_to_value(tree_feature\u001b[38;5;241m=\u001b[39mtree_feature, sql_feature\u001b[38;5;241m=\u001b[39msql_feature)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 164\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(pred_value, target_value ,is_train)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_sample(tree_feature, sql_feature, target_value, mask, \u001b[38;5;28mabs\u001b[39m(pred_value \u001b[38;5;241m-\u001b[39m target_value))\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_value, pred_value\n",
      "File \u001b[0;32m~/project/mayang/HyperQO/NET.py:137\u001b[0m, in \u001b[0;36mTreeNet.loss\u001b[0;34m(self, pred_value, target, optimize)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_value\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 137\u001b[0m loss_value\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_cnt=1\n",
    "list_pred=[]\n",
    "list_loss=[]\n",
    "list_pred_val=[]\n",
    "list_loss_val=[]\n",
    "list_batch_loss=[]\n",
    "list_batch_loss_val=[]\n",
    "# 训练循环\n",
    "batch_num=0\n",
    "for epoch in range(1):  # 例如，训练多个 epochs\n",
    "    loader = Data.DataLoader(dataset=train_set,\n",
    "                                 batch_size=Batch_Size,\n",
    "                                 shuffle=True)\n",
    "    loader_val = Data.DataLoader(dataset=val_set,\n",
    "                                 batch_size=Batch_Size//4,\n",
    "                                 shuffle=True)\n",
    "    loader_val=[x for x in loader_val]\n",
    "    for batch_x, batch_y in loader:\n",
    "        optimizer.zero_grad()  # 每个批次前先清零梯度\n",
    "        batch_loss = 0\n",
    "        batch_loss_val=0\n",
    "        # training process\n",
    "        for num in range(Batch_Size):\n",
    "            sql = queries[batch_x[num]]\n",
    "            target_value = batch_y[num]\n",
    "            plan_json = pgrunner.getCostPlanJson(sql)\n",
    "            sql_vec = workload_embedder.get_embedding([sql])\n",
    "\n",
    "            # 计算损失\n",
    "            loss, pred_val = treenet_model.train(plan_json, sql_vec, target_value, mask,is_train=True)\n",
    "            list_loss.append(loss)\n",
    "            list_pred.append(pred_val)\n",
    "            print(\"training count {} : train loss : {}, pred_val : {}, target_value : {},  diff : {}\".format(run_cnt,loss, pred_val,target_value,abs(pred_val-target_value)))\n",
    "            batch_loss += loss  # 累积批次损失\n",
    "            run_cnt+=1\n",
    "        list_batch_loss.append(batch_loss)\n",
    "        print(\"batch loss : {}\".format(batch_loss / Batch_Size))\n",
    "        # val process  4:1的比例进行验证\n",
    "        for num in range(Batch_Size//4):\n",
    "            # valid process\n",
    "            batch_x_val,batch_y_val=loader_val[batch_num]\n",
    "            sql = queries[batch_x_val[num]]\n",
    "            target_value = batch_y_val[num]\n",
    "            plan_json = pgrunner.getCostPlanJson(sql)\n",
    "            sql_vec = workload_embedder.get_embedding([sql])\n",
    "\n",
    "            # 计算损失\n",
    "            loss, pred_val = treenet_model.train(plan_json, sql_vec, target_value, mask, is_train=False)\n",
    "            list_loss_val.append(loss)\n",
    "            list_pred_val.append(pred_val)\n",
    "            print(\"valid epo : {}, train loss : {}, pred_val : {}, target_value : {},  diff : {}\".format(batch_num,loss, pred_val,target_value,abs(pred_val-target_value)))\n",
    "            batch_loss_val +=loss\n",
    "        list_batch_loss_val.append(batch_loss)\n",
    "        print(\"valid batch loss : {}\".format(batch_loss_val / (Batch_Size//4)))\n",
    "        batch_num+=1  #记录批次batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec80ab0c-b4d8-4c95-af51-cf0646119086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(treenet_model.value_network.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fb7aa9-bc6b-4fc4-a288-9058ace4dedf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型参数\n",
    "model =TreeNet(tree_builder, value_network)\n",
    "model.value_network.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3790d4-b908-4c52-b1a9-23a494d9c460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0175, dtype=torch.float64, grad_fn=<SmoothL1LossBackward0>),\n",
       " tensor(0.4807, grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(plan_json, sql_vec, target_value, mask, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675c13f-7cd9-4b07-a25d-6c3067e5f7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "list_loss=[float(x) for x in list_loss]\n",
    "list_pred=[(x) for x in list_pred]\n",
    "# x轴数据\n",
    "x = list(range(len(list_pred)))\n",
    " \n",
    "# y轴第一条曲线数据\n",
    "y1 = list_loss[::50]\n",
    " \n",
    "# y轴第二条曲线数据\n",
    "y2 = list_pred[::50]\n",
    " \n",
    "# 创建子图对象\n",
    "fig, ax = plt.subplots()\n",
    " \n",
    "# 绘制第一条曲线\n",
    "ax.plot(x, y1, label='Curve 1')\n",
    " \n",
    "# 绘制第二条曲线\n",
    "ax.plot(x, y2, label='Curve 2')\n",
    " \n",
    "# 添加标题、x轴和y轴标签\n",
    "plt.title('Two Curves Plot')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    " \n",
    "# 显示图例\n",
    "plt.legend()\n",
    " \n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409935c7-d8d2-489e-9253-00b418f013bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.502861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.110074</td>\n",
       "      <td>0.397846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.451062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022763</td>\n",
       "      <td>0.461958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.467955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss      pred\n",
       "0  0.010964  0.502861\n",
       "1  0.110074  0.397846\n",
       "2  0.002929  0.451062\n",
       "3  0.022763  0.461958\n",
       "4  0.002416  0.467955"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=pd.DataFrame()\n",
    "res['loss']=[float(x) for x in list_loss]\n",
    "res['pred']=[float(x) for x in list_pred]\n",
    "res.to_csv('training_result.csv')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cedf7d11-1520-4bb3-8a1d-6c2aadf02428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354890ab-68ab-4e79-bc32-12c77c494336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
